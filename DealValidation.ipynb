{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Operations - Lost invalid deals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this code, we will access Pipedrive through the API and capture all Organizations, all Deals, and constantly update the Deals.\n",
    "\n",
    "Next, we will construct the DealValidation table, which will link Organizations with Deals. We will proceed to associate the company's Website with its respective deal.\n",
    "\n",
    "We will apply a Website Validation function to check the website of each Organization and create a list containing all the websites that are not valid. We will then apply a \"lost\" status to their respective deals.\n",
    "\n",
    "This way, any Deal with an invalid website will be removed from the sales funnel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Creating Functions and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packagens\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- API Pipedrive:\n",
    "# https://developers.pipedrive.com/docs/api/v1\n",
    "\n",
    "#Accessing Keys\n",
    "your_token = \"\" #Insert your token here\n",
    "token = f'?api_token={your_token}'\n",
    "pipedriveURL = 'https://snovio-5a30ce.pipedrive.com/api/v1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Creating Functions\n",
    "\n",
    "# Function to transform the URL\n",
    "def make_https(url):\n",
    "    if url and isinstance(url, str):\n",
    "        # Remove \"http://\", \"https://\", and \"www.\"\n",
    "        url = url.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\"www.\", \"\")\n",
    "        # Extract only the domain and site name\n",
    "        parsed_url = urlparse(url)\n",
    "        return f\"https://www.{parsed_url.netloc}{parsed_url.path}\"\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "# Creating a session for persistent connections\n",
    "session = requests.Session()\n",
    "\n",
    "# Creating a get_status function that uses the session\n",
    "def get_status(url):\n",
    "    if url and isinstance(url, str):\n",
    "        try:\n",
    "            # Use the session for persistent connections\n",
    "            response = requests.head(url, timeout=10, allow_redirects=True)\n",
    "            \n",
    "            # Check the status code\n",
    "            if response.status_code == 200 or response.status_code == 403:\n",
    "                return 'valid'\n",
    "            elif response.status_code == 404:\n",
    "                return 'invalid'\n",
    "            else:\n",
    "                return 'other HTML response'\n",
    "        except requests.Timeout:\n",
    "            return 'timed out'\n",
    "        except Exception as e:\n",
    "            # Handle exceptions, if any\n",
    "            return 'exception'\n",
    "    else:\n",
    "        return 'no website'\n",
    "\n",
    "def get_pipedrive(get, params=None):\n",
    "    url = f\"{pipedriveURL}{get}{token}\"             #Building the url\n",
    "    response = requests.get(url, params=params)     #using request\n",
    "    return response.json()\n",
    "\n",
    "def put_pipedrive(put, params=None):\n",
    "    url = f\"{pipedriveURL}{put}{token}\"             #Building the url\n",
    "    response = requests.put(url, data=params)     #using request\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x179c61d6cc0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------- Creating Tables\n",
    "#Create Database\n",
    "db_path = os.path.join(\"data\", \"pipedrive.db\")\n",
    "connection = sqlite3.connect(db_path)\n",
    "\n",
    "#Organization Table\n",
    "CREATE_ORG_TABLE = \"\"\" \n",
    "CREATE TABLE IF NOt EXISTS Organization (\n",
    "    org_id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    website TEXT,\n",
    "    page INTEGER\n",
    ");\"\"\"\n",
    "connection.execute(CREATE_ORG_TABLE)\n",
    "\n",
    "#DEALS table\n",
    "CREATE_DEAL_TABLE = \"\"\" \n",
    "CREATE TABLE IF NOT EXISTS DEALS (\n",
    "   deal_id INTEGER PRIMARY KEY,\n",
    "    org_id INTEGER,\n",
    "    title TEXT,\n",
    "    owner TEXT,\n",
    "    value INTEGER,\n",
    "    pipeline_id INTEGER,\n",
    "    stage_order_nr INTEGER,\n",
    "    add_time DATETIME,\n",
    "    update_time DATETIME,\n",
    "    label TEXT,\n",
    "    status TEXT,\n",
    "    lost_reason TEXT,\n",
    "    page_number INTEGER\n",
    ");\"\"\"\n",
    "\n",
    "connection.execute(CREATE_DEAL_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Get Pipedrive Data throught API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Getting all Organizations, \n",
    "#starting from the max value page\n",
    "cursor = connection.cursor()\n",
    "cursor.execute('SELECT MAX(page) FROM Organization')\n",
    "max_page = cursor.fetchone()[0]\n",
    "morepages = True\n",
    "website_key = '4873360cacbe0b1dd893824bc11938dfd53ec655' #Insert your website_key if necessary, see more in https://pipedrive.readme.io/docs/core-api-concepts-about-pipedrive-api\n",
    "\n",
    "#-- Getting all organization\n",
    "while morepages == True:\n",
    "    get = 'organizations'\n",
    "    params = {\"limit\":\"500\", \"start\":[max_page]}\n",
    "    org = get_pipedrive(get,params)\n",
    "    if org['data'] is not None:\n",
    "        morepages = org['additional_data']['pagination']['more_items_in_collection']\n",
    "        with connection:\n",
    "            for i in range(0,len(org['data'])):\n",
    "                insert_sql = \"\"\" INSERT OR REPLACE INTO Organization (org_id,name,website,page) VALUES (?,?,?,?);\"\"\"\n",
    "                connection.execute(insert_sql, (org['data'][i]['id'],org['data'][i]['name'], org['data'][i][website_key],max_page))\n",
    "\n",
    "            max_page += 500\n",
    "    else:\n",
    "        morepages = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Getting All deals\n",
    "\n",
    "#Parameters\n",
    "number = 0 #Use it when want to start a new table, just remove the page_number variable to this one\n",
    "morepages = True\n",
    "\n",
    "#Getting Last page\n",
    "cursor = connection.cursor()\n",
    "cursor.execute('SELECT MAX(page_number) FROM DEALS')\n",
    "page_number = cursor.fetchone()[0]\n",
    "\n",
    "#--- \n",
    "while morepages == True:\n",
    "    #Capturing recently Deals\n",
    "    get = 'deals'\n",
    "    params = {\"limit\":\"500\", \"start\":[page_number]}\n",
    "    deal = get_pipedrive(get,params)\n",
    "    if deal['data'] is not None:\n",
    "        morepages = deal['additional_data']['pagination']['more_items_in_collection']\n",
    "        with connection:\n",
    "            for i in range(0,len(deal['data'])):\n",
    "                #Check if org_id_value is not null\n",
    "                org_id_value = deal['data'][i]['org_id']['value'] if deal['data'][i]['org_id'] and deal['data'][i]['org_id']['value'] is not None else 'no value'\n",
    "\n",
    "                insert_sql = \"\"\"\n",
    "                    INSERT OR REPLACE INTO DEALS (\n",
    "                        deal_id,\n",
    "                        org_id,\n",
    "                        title,\n",
    "                        owner,\n",
    "                        value,\n",
    "                        pipeline_id,\n",
    "                        stage_order_nr,\n",
    "                        add_time,\n",
    "                        update_time,\n",
    "                        label,\n",
    "                        status,\n",
    "                        lost_reason,\n",
    "                        page_number\n",
    "                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,?);\n",
    "                \"\"\"\n",
    "\n",
    "                # Par√¢metros\n",
    "                params = (\n",
    "                    deal['data'][i]['id'],\n",
    "                    org_id_value,\n",
    "                    deal['data'][i]['title'],\n",
    "                    deal['data'][i]['owner_name'],\n",
    "                    deal['data'][i]['value'],\n",
    "                    deal['data'][i]['pipeline_id'],\n",
    "                    deal['data'][i]['stage_id'],\n",
    "                    deal['data'][i]['add_time'],\n",
    "                    deal['data'][i]['update_time'],\n",
    "                    deal['data'][i]['label'],\n",
    "                    deal['data'][i]['status'],\n",
    "                    deal['data'][i]['lost_reason'],\n",
    "                    deal['additional_data']['pagination']['start']\n",
    "                )\n",
    "\n",
    "                connection.execute(insert_sql,params)\n",
    "        page_number += 500\n",
    "    else:\n",
    "        morepages = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------ Updating the Deals\n",
    "#Here, numbe_page in database DEALS will be null to indicate that was updated\n",
    "\n",
    "#Parameters\n",
    "morepages = True\n",
    "page_number = 0\n",
    "\n",
    "#Get the last update timestamp\n",
    "cursor = connection.cursor()\n",
    "cursor.execute('SELECT MAX(update_time) FROM DEALS') #Getting the last time deal updated from SQL\n",
    "timestamp_str = cursor.fetchone()[0] #The value will return a string\n",
    "timestamp_pipedrive = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S') #Converting the string into date\n",
    "timestamp_subtracted = timestamp_pipedrive - timedelta(hours=4) #Subtracting 4 hours to get in Brazil time zone\n",
    "timestamp = timestamp_subtracted.strftime('%Y-%m-%d %H:%M:%S') #Converted to last time deal update in Brazil time zone\n",
    "\n",
    "#--- execute the loop\n",
    "while morepages == True:\n",
    "    # Capturing recently Deals\n",
    "    get = 'recents'\n",
    "    params = {\"since_timestamp\":[timestamp],\"limit\":\"500\", \"start\":[page_number], \"items\":\"deal\"}\n",
    "    recent_data = get_pipedrive(get, params)  # Renomeando a vari√°vel para evitar conflito com 'deal'\n",
    "    if recent_data['data'] is not None:\n",
    "        morepages = recent_data['additional_data']['pagination']['more_items_in_collection']\n",
    "        with connection:\n",
    "            for i in range(0, len(recent_data['data'])):\n",
    "                # Check if org_id_value is not null\n",
    "                org_id_value = recent_data['data'][i]['data']['org_id'] if recent_data['data'][i]['data']['org_id'] and recent_data['data'][i]['data']['org_id'] is not None else 'no value'\n",
    "\n",
    "                insert_sql = \"\"\"\n",
    "                    INSERT OR REPLACE INTO DEALS (\n",
    "                        deal_id,\n",
    "                        org_id,\n",
    "                        title,\n",
    "                        owner,\n",
    "                        value,\n",
    "                        pipeline_id,\n",
    "                        stage_order_nr,\n",
    "                        add_time,\n",
    "                        update_time,\n",
    "                        label,\n",
    "                        status,\n",
    "                        lost_reason\n",
    "                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "                \"\"\"\n",
    "\n",
    "                # Par√¢metros\n",
    "                params = (\n",
    "                    recent_data['data'][i]['data']['id'],\n",
    "                    org_id_value,\n",
    "                    recent_data['data'][i]['data']['title'],\n",
    "                    recent_data['data'][i]['data']['owner_name'],\n",
    "                    recent_data['data'][i]['data']['value'],\n",
    "                    recent_data['data'][i]['data']['pipeline_id'],\n",
    "                    recent_data['data'][i]['data']['stage_id'],\n",
    "                    recent_data['data'][i]['data']['add_time'],\n",
    "                    recent_data['data'][i]['data']['update_time'],\n",
    "                    recent_data['data'][i]['data']['label'],\n",
    "                    recent_data['data'][i]['data']['status'],\n",
    "                    recent_data['data'][i]['data']['lost_reason']\n",
    "                )\n",
    "\n",
    "                connection.execute(insert_sql, params)\n",
    "        page_number += 500\n",
    "    else:\n",
    "        morepages = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Generate DealValidation table and the Dataframe to validate domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x179c5ef8a40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DROP TABLE IF IT EXISTS\n",
    "connection.execute(\"DROP TABLE IF EXISTS DEALVALIDATION;\")\n",
    "\n",
    "# CREATE NEW TABLE TO JOIN THE NEW INFORMATIONS FROM DEALS AND ORGANIZATIONS\n",
    "connection.execute(\"\"\"\n",
    "    CREATE TABLE DEALVALIDATION AS\n",
    "    SELECT\n",
    "        rd.org_id,\n",
    "        o.name,\n",
    "        o.website,\n",
    "        rd.deal_id,\n",
    "        rd.title,\n",
    "        rd.owner,\n",
    "        rd.pipeline_id,\n",
    "        rd.stage_order_nr,\n",
    "        rd.add_time,\n",
    "        rd.update_time,\n",
    "        rd.status,\n",
    "        rd.lost_reason\n",
    "    FROM\n",
    "        DEALS rd\n",
    "    LEFT JOIN\n",
    "        Organization o ON rd.org_id = o.org_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Filtering the table Dealvalidation for testing and query for dataframe\n",
    "query = f\"\"\" \n",
    "SELECT *\n",
    "FROM DEALVALIDATION\n",
    "WHERE\n",
    "    pipeline_id = 33\n",
    "    AND stage_order_nr = 196\n",
    "    AND add_time >= '{timestamp}'\n",
    "    AND status = 'open';\n",
    "    \"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a standard URL format\n",
    "df['URL'] = df['website'].apply(make_https)\n",
    "\n",
    "#--- Apply get_status function \n",
    "\n",
    "# Working with DataFrame df\n",
    "chunk_size = 4  # Try different chunk sizes\n",
    "chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Process the chunks in parallel using ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(lambda chunk: chunk.assign(validation=chunk['URL'].map(get_status)), chunks))\n",
    "\n",
    "# Concatenate the results\n",
    "df = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only validation, websit and deal_id column an then filtering just to not valids\n",
    "df1 = df[['validation','deal_id','URL','title']]\n",
    "df1 = df1[df1['validation'].isin(['valid', 'Other HTML']) == False]\n",
    "df1.tail()\n",
    "#df.to_excel(\"DealInvalid.xlsx\", index = False) #to export\n",
    "\n",
    "invalid_ids = df1['deal_id'].tolist()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Apply lost to all invalid deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## APPLYING LOST TO ALL INVALID DEALS\n",
    "#--- Aplying lost api pipedrive to the deals that contains invalid URL\n",
    "\n",
    "for invalid in invalid_ids:\n",
    "    put = f'deals/{invalid}'\n",
    "    params = {\"status\":'lost', \"lost_reason\": \"Invalid Website\" }\n",
    "    put_pipedrive(put,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
